# Diagrama del Flujo de Aprendizaje por Refuerzo

## ğŸ”„ Ciclo Principal del Algoritmo Q-Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INICIO DEL EPISODIO                          â”‚
â”‚                  Resetear Entorno (t=0)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  OBSERVAR ESTADO s_t  â”‚
              â”‚  (4 valores continuos)â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   DISCRETIZAR        â”‚
              â”‚   s_t â†’ s_discrete   â”‚
              â”‚   (10 bins Ã— 4 dim)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   SELECCIÃ“N DE ACCIÃ“N         â”‚
         â”‚   (PolÃ­tica Îµ-greedy)         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ random < Îµ ?  â”‚               â”‚
         â”‚    SI    â”‚    NO              â”‚
         â”‚     â†“    â”‚    â†“               â”‚
         â”‚ EXPLORAR â”‚ EXPLOTAR           â”‚
         â”‚ (random) â”‚ (max Q)            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  EJECUTAR ACCIÃ“N a_t  â”‚
              â”‚  en el entorno        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   OBSERVAR RESULTADOS         â”‚
         â”‚   - Recompensa: r_{t+1}       â”‚
         â”‚   - Nuevo estado: s_{t+1}     â”‚
         â”‚   - Terminado: done           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      ACTUALIZACIÃ“N DE TABLA Q            â”‚
         â”‚                                          â”‚
         â”‚  Q(s_t, a_t) â† Q(s_t, a_t) +            â”‚
         â”‚                                          â”‚
         â”‚  Î± Ã— [r_{t+1} + Î³Ã—max Q(s_{t+1}, a') -  â”‚
         â”‚                      Q(s_t, a_t)]        â”‚
         â”‚                                          â”‚
         â”‚  Donde:                                  â”‚
         â”‚  â€¢ Î± = 0.1  (learning rate)              â”‚
         â”‚  â€¢ Î³ = 0.99 (discount factor)            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  REGISTRAR MÃ‰TRICAS      â”‚
         â”‚  - Recompensa acumulada  â”‚
         â”‚  - Pasos del episodio    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Â¿Terminado? â”‚
              â”‚   (done?)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”˜
                NO   â”‚   â”‚  SI
                     â”‚   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
        â”‚ t = t + 1                     â”‚
        â”‚ (siguiente paso)              â”‚
        â”‚                               â”‚
        â””â”€â”€â”                         â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                         â”‚  DECAER EPSILON â”‚
           â”‚                         â”‚  Îµ = Îµ Ã— 0.995  â”‚
           â”‚                         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â”‚                               â–¼
           â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                    â”‚ Â¿MÃ¡s episodios?      â”‚
           â”‚                    â”‚ (episodio < 1000?)   â”‚
           â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                      SI   â”‚       â”‚  NO
           â”‚                           â”‚       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                                                â”‚
                                                â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  FIN DEL          â”‚
                                    â”‚  ENTRENAMIENTO   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Espacios de Estados y Acciones

```
ESTADO (ObservaciÃ³n continua â†’ Discretizada)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚  s = [posiciÃ³n_carro, velocidad_carro,                  â”‚
â”‚       Ã¡ngulo_poste, velocidad_angular]                  â”‚
â”‚                                                          â”‚
â”‚  Rangos continuos:                                       â”‚
â”‚  â€¢ posiciÃ³n_carro:     [-4.8,  4.8]                     â”‚
â”‚  â€¢ velocidad_carro:    [-âˆ,    âˆ]  â†’ [-4.0, 4.0]       â”‚
â”‚  â€¢ Ã¡ngulo_poste:       [-0.418, 0.418] rad (~Â±24Â°)     â”‚
â”‚  â€¢ velocidad_angular:  [-âˆ,    âˆ]  â†’ [-4.0, 4.0]       â”‚
â”‚                                                          â”‚
â”‚  DiscretizaciÃ³n: 10 bins por dimensiÃ³n                   â”‚
â”‚  Estados totales: 10â´ = 10,000 posibles                 â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   DISCRETIZACIÃ“N       â”‚
              â”‚   Ejemplo:             â”‚
              â”‚   s_continuo =         â”‚
              â”‚   [0.5, 1.2, 0.1, -0.8]â”‚
              â”‚         â†“              â”‚
              â”‚   s_discreto =         â”‚
              â”‚   (6, 7, 6, 3)         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   TABLA Q              â”‚
              â”‚                        â”‚
              â”‚   Q[(6,7,6,3), 0] = 45 â”‚
              â”‚   Q[(6,7,6,3), 1] = 52 â”‚
              â”‚                        â”‚
              â”‚   AcciÃ³n Ã³ptima: 1     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ACCIONES (Discretas)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚  a = 0  â†’  Empujar IZQUIERDA   â†       â”‚
â”‚  a = 1  â†’  Empujar DERECHA     â†’       â”‚
â”‚                                         â”‚
â”‚  Efecto: Fuerza de 10N aplicada        â”‚
â”‚         al carro en direcciÃ³n elegida  â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š EvoluciÃ³n del Aprendizaje

```
RECOMPENSA POR EPISODIO
    500 â”‚                              â•±â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”â–”
        â”‚                             â•±
    400 â”‚                        â•±â–”â–”â–”â–”
        â”‚                    â•±â–”â–”â–”
    300 â”‚                â•±â–”â–”â–”
        â”‚            â•±â–”â–”â–”
    200 â”‚       â•±â–”â–”â–”â–”
        â”‚    â•±â–”â–”
    100 â”‚ â•±â–”â–”
        â”‚â–”
      0 â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€
        0    100   200   300   400   500   600   700   800   900  1000
                              EPISODIOS

        FASE 1      â”‚    FASE 2    â”‚      FASE 3
        ExploraciÃ³n â”‚  Aprendizaje â”‚   Convergencia
        Îµ = 1.0â†’0.36â”‚  Îµ = 0.36â†’0.05â”‚  Îµ = 0.05â†’0.01


EPSILON (Tasa de ExploraciÃ³n)
    1.0 â”‚â–”â–”â–”â–„
        â”‚    â–€â–€â–€â–„
    0.8 â”‚        â–€â–€â–„
        â”‚           â–€â–€â–„
    0.6 â”‚              â–€â–€â–„
        â”‚                 â–€â–€â–„
    0.4 â”‚                    â–€â–€â–„
        â”‚                       â–€â–€â–„
    0.2 â”‚                          â–€â–€â–„
        â”‚                             â–€â–€â–€â–„___________
    0.0 â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€
        0    100   200   300   400   500   600   700   800   900  1000
                              EPISODIOS
```

---

## ğŸ§  Tabla Q - RepresentaciÃ³n

```
ESTRUCTURA DE LA TABLA Q
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                        â”‚
â”‚  Estado Discreto        â”‚  AcciÃ³n 0  â”‚  AcciÃ³n 1      â”‚
â”‚  (tupla de 4 valores)   â”‚  (izq)     â”‚  (der)         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  (5, 5, 5, 5)          â”‚   45.2     â”‚   38.7         â”‚
â”‚  (5, 5, 5, 6)          â”‚   42.8     â”‚   47.3         â”‚
â”‚  (5, 5, 6, 5)          â”‚   38.1     â”‚   52.9  â† MAX  â”‚
â”‚  (5, 6, 5, 5)          â”‚   43.7     â”‚   41.2         â”‚
â”‚  ...                   â”‚   ...      â”‚   ...          â”‚
â”‚  (estados explorados: ~2500 de 10,000 posibles)       â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INICIALIZACIÃ“N: Q(s,a) = 0 para todo s, a

ACTUALIZACIÃ“N EJEMPLO:
  Estado actual: (5, 5, 6, 5)
  AcciÃ³n tomada: 1 (derecha)
  Recompensa: +1
  Siguiente estado: (5, 6, 6, 5)
  
  Q_actual = 52.9
  Q_siguiente_max = max(Q[(5,6,6,5), :]) = 48.5
  
  Q_nuevo = 52.9 + 0.1 Ã— [1 + 0.99Ã—48.5 - 52.9]
          = 52.9 + 0.1 Ã— [-3.915]
          = 52.5085
```

---

## ğŸ® Entorno CartPole - VisualizaciÃ³n

```
                  ğŸ”´ POSTE
                   â”‚
                   â”‚
                   â”‚  â†— Î¸ (Ã¡ngulo)
                   â”‚â†™
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Eje vertical
                  â•±â”‚â•²
                 â•± â”‚ â•²
                â•±  â”‚  â•²
               â•±   â”‚   â•²
              â•±    â”‚    â•²
             â•±     â”‚     â•²
            â•±      â”‚      â•²
           â•±       â”‚       â•²
        ğŸŸ¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ğŸŸ¦  â† Carro
                   â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â† Pista
        â†‘                     â†‘
      -2.4m                 +2.4m
      (lÃ­mite)              (lÃ­mite)

ESTADO:
  â€¢ PosiciÃ³n del carro: x
  â€¢ Velocidad del carro: áº‹
  â€¢ Ãngulo del poste: Î¸
  â€¢ Velocidad angular: Î¸Ì‡

DINÃMICA:
  â€¢ Gravedad actÃºa sobre el poste
  â€¢ Fuerza aplicada al carro (10N izq/der)
  â€¢ FÃ­sica realista (fricciÃ³n, masa)

OBJETIVO:
  Mantener â”‚Î¸â”‚ < 12Â° el mayor tiempo posible
  (mÃ¡ximo 500 pasos = recompensa de 500)
```

---

## ğŸ”§ Flujo de Archivos

```
ENTRENAMIENTO                    VISUALIZACIÃ“N WEB
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rl_agent_       â”‚             â”‚                      â”‚
â”‚ cartpole.py     â”‚  ejecuta    â”‚      Flask App       â”‚
â”‚                 â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚      (app.py)        â”‚
â”‚ â€¢ QLearningAgentâ”‚             â”‚                      â”‚
â”‚ â€¢ train()       â”‚             â”‚ â€¢ Carga modelo       â”‚
â”‚ â€¢ evaluate()    â”‚             â”‚ â€¢ Extrae mÃ©tricas    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â€¢ Genera HTML        â”‚
         â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ genera                          â”‚
         â”‚                                 â”‚ muestra
         â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         â”‚     â”‚                        â”‚
â”‚  ARCHIVOS GENERADOS     â”‚     â”‚   NAVEGADOR WEB        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                         â”‚     â”‚                        â”‚
â”‚  1. modelo_rl_          â”‚     â”‚  /conceptos_refuerzo   â”‚
â”‚     cartpole.pkl        â”‚â—€â”€â”€â”€â”¤  â€¢ TeorÃ­a              â”‚
â”‚     (tabla Q + params)  â”‚leer â”‚  â€¢ Referencias APA     â”‚
â”‚                         â”‚     â”‚                        â”‚
â”‚  2. static/             â”‚     â”‚  /caso_practico_       â”‚
â”‚     rl_training_        â”‚â—€â”€â”€â”€â”¤  refuerzo              â”‚
â”‚     rewards.png         â”‚leer â”‚  â€¢ DescripciÃ³n entorno â”‚
â”‚                         â”‚     â”‚  â€¢ ParÃ¡metros          â”‚
â”‚  3. static/             â”‚     â”‚  â€¢ MÃ©tricas            â”‚
â”‚     rl_training_        â”‚â—€â”€â”€â”€â”¤  â€¢ GrÃ¡ficas            â”‚
â”‚     distributions.png   â”‚leer â”‚  â€¢ AnÃ¡lisis            â”‚
â”‚                         â”‚     â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

```
CRITERIOS DE CONVERGENCIA
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                    â”‚
â”‚  âœ… EXCELENTE (Recompensa > 450)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ â€¢ PolÃ­tica Ã³ptima aprendida         â”‚           â”‚
â”‚  â”‚ â€¢ Poste vertical >90% del tiempo    â”‚           â”‚
â”‚  â”‚ â€¢ Varianza baja entre episodios     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                    â”‚
â”‚  âš ï¸  PARCIAL (Recompensa 300-450)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ â€¢ Aprendizaje incompleto            â”‚           â”‚
â”‚  â”‚ â€¢ Aumentar episodios de entrenamientoâ”‚          â”‚
â”‚  â”‚ â€¢ Ajustar hiperparÃ¡metros           â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                    â”‚
â”‚  âŒ INSUFICIENTE (Recompensa < 300)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ â€¢ No ha convergido                  â”‚           â”‚
â”‚  â”‚ â€¢ Revisar implementaciÃ³n            â”‚           â”‚
â”‚  â”‚ â€¢ Incrementar recursos de cÃ³mputo   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Autor:** Proyecto Machine Learning - Universidad de Cundinamarca  
**Fecha:** Noviembre 2025
