# Gu√≠a R√°pida: Aprendizaje por Refuerzo - Q-Learning

## ‚ö° Inicio R√°pido (5 minutos)

### Paso 1: Instalar Dependencias
```powershell
cd e:\Github\Casos-de-Uso-de-Machine-Learning-Supervisado
pip install gymnasium numpy matplotlib Flask
```

### Paso 2: Entrenar el Agente
```powershell
python Proyecto\rl_agent_cartpole.py
```

**Salida esperada:**
```
====================================================================
Iniciando entrenamiento del agente Q-Learning
====================================================================
Par√°metros:
  - Episodios: 1000
  - Learning rate (Œ±): 0.1
  - Discount factor (Œ≥): 0.99
  - Epsilon inicial (Œµ): 1.0
  ...

Episodio 100/1000 | Recompensa promedio (√∫ltimos 100): 23.45 | Epsilon: 0.6050
Episodio 200/1000 | Recompensa promedio (√∫ltimos 100): 87.32 | Epsilon: 0.3660
...
Episodio 1000/1000 | Recompensa promedio (√∫ltimos 100): 487.25 | Epsilon: 0.0100

====================================================================
Entrenamiento completado!
====================================================================
```

### Paso 3: Ver Resultados en la Web
```powershell
python app.py
```

Abrir navegador: **http://localhost:5000/caso_practico_refuerzo**

---

## üìã Verificaci√≥n R√°pida

### ‚úÖ Archivos que deber√≠an generarse:
- [ ] `Proyecto/modelo_rl_cartpole.pkl` (~100KB)
- [ ] `Proyecto/static/rl_training_rewards.png`
- [ ] `Proyecto/static/rl_training_distributions.png`

### ‚úÖ M√©tricas de √©xito:
- Recompensa promedio final: **> 450** ‚úÖ
- Estados explorados: **~2000-3000**
- Tiempo de entrenamiento: **2-5 minutos**

---

## üéØ Navegaci√≥n en la Web

1. **Conceptos B√°sicos:** `/conceptos_refuerzo`
   - Teor√≠a del Aprendizaje por Refuerzo
   - Componentes del sistema
   - Algoritmos principales
   - Referencias acad√©micas (APA 7)

2. **Caso Pr√°ctico:** `/caso_practico_refuerzo`
   - Descripci√≥n del entorno CartPole
   - Par√°metros del algoritmo
   - Resultados del entrenamiento
   - Gr√°ficas interactivas
   - An√°lisis de convergencia

---

## üîß Soluci√≥n de Problemas

### Problema: "gymnasium" no encontrado
```powershell
pip install gymnasium
```

### Problema: Error de matplotlib
```powershell
pip install --upgrade matplotlib
```

### Problema: Puerto 5000 ocupado
En `app.py`, cambiar √∫ltima l√≠nea:
```python
app.run(debug=True, port=5001)
```

---

## üìñ Documentaci√≥n Completa

Para m√°s detalles, consultar:
- **`Proyecto/README_RL.md`** - Documentaci√≥n t√©cnica completa
- **`RESUMEN_RL.md`** - Resumen del trabajo realizado

---

## üéì Conceptos Clave Implementados

| Concepto | Implementaci√≥n |
|----------|----------------|
| **Q-Learning** | Tabla Q con actualizaci√≥n de Bellman |
| **Œµ-greedy** | Exploraci√≥n decreciente (1.0 ‚Üí 0.01) |
| **Discretizaci√≥n** | 10 bins √ó 4 dimensiones = 10K estados |
| **Visualizaci√≥n** | Matplotlib + Bootstrap 5 |
| **Persistencia** | Pickle para guardar modelo |

---

## üí° Pr√≥ximos Pasos Sugeridos

1. **Experimentar con par√°metros:**
   - Cambiar `learning_rate` (0.05, 0.2)
   - Ajustar `bins` (5, 15, 20)
   - Modificar `epsilon_decay` (0.99, 0.995, 0.999)

2. **Probar otros entornos:**
   - MountainCar-v0
   - Acrobot-v1
   - LunarLander-v2

3. **Implementar mejoras:**
   - SARSA algorithm
   - Experience Replay
   - Deep Q-Network (DQN)

---

**¬°Listo para empezar! üöÄ**
